{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e51ed2-cf8b-4f47-8b1a-da100de6e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 16:27:12.221540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-08 16:27:12.221638: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\n",
    "# from transformers import InputExample, InputFeatures\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from importlib import reload\n",
    "# from src import embeddings_filter\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369d336f-2648-49ba-afd8-42bdb36c95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0afdee-4474-4e26-b3b0-5cf876c90cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 16:28:17.851254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-08 16:28:17.851470: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-08 16:28:17.851584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noto.epfl.ch): /proc/driver/nvidia/version does not exist\n",
      "2021-11-08 16:28:17.852519: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929df3ab-797d-4277-a80e-abcc0851879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fa0542-4a3a-46f4-a9ec-1c40421fdc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\n",
      "\n",
      "  Tokens: ['are', 'we', 'able', 'to', 'predict', 'the', 'stock', 'market', 'using', 'the', 'sentiment', 'expressed', 'by', 'famous', 'people', '?', 'ada', '##del', '##ta', '-', 'q', 'is', 'here', 'to', 'answer', 'your', 'question']\n",
      "\n",
      "  Token IDs: [2024, 2057, 2583, 2000, 16014, 1996, 4518, 3006, 2478, 1996, 15792, 5228, 2011, 3297, 2111, 1029, 15262, 9247, 2696, 1011, 1053, 2003, 2182, 2000, 3437, 2115, 3160]\n"
     ]
    }
   ],
   "source": [
    "# Test if the tokenizer works\n",
    "sample_txt = \"Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\"\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496598ff-1154-4a31-94a5-0132131b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "k = 0\n",
    "with bz2.open('data/final_filtered.json.bz2', 'rb') as s_file:\n",
    "    while True:\n",
    "        try:\n",
    "            d =  json.loads(next(s_file))\n",
    "            # print(d['quotation'])\n",
    "            sentences.append(d)\n",
    "            k+=1\n",
    "            if k == 1000:\n",
    "                break\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae67b9fe-c9d4-445d-a12a-828d1a7c7ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    However, due to sharp decline in KG-D6 gas pro...\n",
       "1    [ Young savers' ] parents and grandparents are...\n",
       "2    However, we cannot share any specific details ...\n",
       "3    HUD did not drop the complaint, but insisted t...\n",
       "4    2014 was a milestone year for us on many front...\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quotation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a507de91-b7e0-4eca-93d1-cd0bd0f17295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782  \n",
       "1  young saver parent grandpar use higher interes...           0.000000  \n",
       "2  howev share specif detail stage we provid info...           0.170075  \n",
       "3        hud drop complaint insist still issu code\\n           0.286240  \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb79544-6586-43a4-9cb0-19204f2a2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#     'data/final_filtered.json.bz2', batch_size=30000, validation_split=0.2, \n",
    "#     subset='training', seed=123)\n",
    "# test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#     '../data/final_filtered.json.bz2', batch_size=1000000)\n",
    "# We might need to fist read in the dataset as json and transform it into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621b8fd-d142-4c41-9e3d-2765d43ae41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test.take(1):\n",
    "#     test_feat = i[0].numpy()\n",
    "\n",
    "# test = pd.DataFrame([train_feat]).T\n",
    "# test.columns = ['DATA_COLUMN']\n",
    "# test['DATA_COLUMN'] = test['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a18a4c9-39ea-4f7b-880c-ad98c88f2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: However, due to sharp decline in KG-D6 gas production not only could gas not be allocated, to new gas-based projects, but the commissioned capacity that KG-D6 gas allocation also get stranded,\n",
      "\n",
      "  Tokens: ['however', ',', 'due', 'to', 'sharp', 'decline', 'in', 'kg', '-', 'd', '##6', 'gas', 'production', 'not', 'only', 'could', 'gas', 'not', 'be', 'allocated', ',', 'to', 'new', 'gas', '-', 'based', 'projects', ',', 'but', 'the', 'commissioned', 'capacity', 'that', 'kg', '-', 'd', '##6', 'gas', 'allocation', 'also', 'get', 'stranded', ',']\n",
      "\n",
      "  Token IDs: [2174, 1010, 2349, 2000, 4629, 6689, 1999, 4705, 1011, 1040, 2575, 3806, 2537, 2025, 2069, 2071, 3806, 2025, 2022, 11095, 1010, 2000, 2047, 3806, 1011, 2241, 3934, 1010, 2021, 1996, 4837, 3977, 2008, 4705, 1011, 1040, 2575, 3806, 16169, 2036, 2131, 15577, 1010]\n"
     ]
    }
   ],
   "source": [
    "#Try tokenization of one sentence in dataframe\n",
    "sample_txt = df.iloc[0]['quotation']\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94eacbc-afae-4384-a860-5e69cd3e7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(test, DATA_COLUMN): \n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                                 text_a = x[DATA_COLUMN], \n",
    "                                                                 text_b = None), axis=1)\n",
    "    return validation_InputExamples\n",
    "\n",
    "validation_InputExamples = convert_data_to_examples(df, 'quotation')\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=256):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "                #, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                #f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "#LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db572d91-1cc8-4932-a631-0a2b46ed8240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4329/2714608680.py\", line 1, in <module>\n",
      "    validation_InputExamples = convert_data_to_examples(test, DATA_COLUMN)\n",
      "NameError: name 'convert_data_to_examples' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 392, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 381, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 357, in normpath\n",
      "    for comp in comps:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4329/2714608680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_InputExamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_InputExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_examples' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ada-2021-project-adadelta-q/project_env/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# validation_InputExamples = convert_data_to_examples(test, DATA_COLUMN)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aff12d01-e706-4a51-aa4e-6b99f5e709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\n",
    "                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie',\n",
    "                 'the market is indifferent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a10710-d49e-48b4-863b-746c3f16320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.39626688 0.60373306]\n",
      " [0.4094616  0.59053844]], shape=(2, 2), dtype=float32)\n",
      "This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n",
      " Positive\n",
      "One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie : \n",
      " Positive\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "print(tf_predictions)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db3ddda-0adc-4f3c-b449-da1c1d2d5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09925a42-4db6-46eb-a330-906e4935f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    if scores[\"neg\"] > scores[\"pos\"] and scores[\"neg\"] > scores[\"pos\"]:\n",
    "        return 'negative'\n",
    "    elif scores[\"pos\"] > scores[\"neg\"] and scores[\"pos\"] > scores[\"neu\"]:\n",
    "        return 'positive'\n",
    "    return 'neutral'\n",
    "\n",
    "# labels = vader_sentiment_result(pred_sentences)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     print(pred_sentences[i], \": \\n\", vader_sentiment_result(pred_sentences[i]))\n",
    "# train_set[\"vader_result\"] = train_set[\"review\"].apply(lambda x: vader_sentiment_result(x))\n",
    "# valid_set[\"vader_result\"] = valid_set[\"review\"].apply(lambda x: vader_sentiment_result(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf4c0b5-c3cf-4225-9059-7d2a625afd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d19bf5f3ef41999b13c56f09a571ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f787b2a81642d69ba732d829659034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317cf76db9764277905432c821355585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:02<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d95cbb72344b5daa28e1af65ceb8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4c65ab44fe4329b501bd4a313f8894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "a_review = pred_sentences[1]\n",
    "\n",
    "res = classifier(a_review, the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1251dc-fadb-4e56-b4ee-9429c68c7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie', 'labels': ['negative', 'positive'], 'scores': [0.9947044253349304, 0.005295595154166222]}\n"
     ]
    }
   ],
   "source": [
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5b5f84-d57a-4bdf-a3c4-4d0b67b082a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [2:43:14,  9.79s/it]\n"
     ]
    }
   ],
   "source": [
    "hypothesis_template = \"The sentiment of this quote is {}.\"\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "vader_sent = []\n",
    "one_shot_sent = []\n",
    "\n",
    "for idx, item in tqdm(df.iterrows()):\n",
    "    vader_sent.append(vader_sentiment_result(item['quotation']))\n",
    "    one_shot_sent.append(classifier(item['quotation'], the_labels, hypothesis_template = hypothesis_template, multi_label=True)['labels'][0])\n",
    "\n",
    "df['vader sentiment'] = vader_sent\n",
    "df['one-shot sentiment'] = one_shot_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3019291f-a4fb-4681-9a6b-80fb099ee528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>vader sentiment</th>\n",
       "      <th>one-shot sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \\\n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782   \n",
       "1  young saver parent grandpar use higher interes...           0.000000   \n",
       "2  howev share specif detail stage we provid info...           0.170075   \n",
       "3        hud drop complaint insist still issu code\\n           0.286240   \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876   \n",
       "\n",
       "  vader sentiment one-shot sentiment  \n",
       "0         neutral           negative  \n",
       "1         neutral           positive  \n",
       "2         neutral           negative  \n",
       "3         neutral           negative  \n",
       "4         neutral           positive  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9934e7c-a71e-4ac6-ab0e-4ca91425df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='out.csv')  \n",
    "df.to_csv('out.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63f6fe-3896-4eb1-9098-3163b95d997e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
