{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e51ed2-cf8b-4f47-8b1a-da100de6e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 17:09:41.687551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 17:09:41.687589: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries. Note, in order to be able to run this cell you must have installed the libraires listed \n",
    "# in the requirements.txt file\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures, pipeline\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import json\n",
    "import bz2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668622b-798a-4e53-a70c-eee485973db3",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "In the pipeline, the sentiment classification task is after filtering out irrelevant sentences. We thus need to load the dataset back into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496598ff-1154-4a31-94a5-0132131b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "k = 0\n",
    "with bz2.open('data/final_filtered.json.bz2', 'rb') as s_file:\n",
    "    while True:\n",
    "        try:\n",
    "            d =  json.loads(next(s_file))\n",
    "            # print(d['quotation'])\n",
    "            sentences.append(d)\n",
    "            k+=1\n",
    "            if k == 1000:\n",
    "                break\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae67b9fe-c9d4-445d-a12a-828d1a7c7ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    However, due to sharp decline in KG-D6 gas pro...\n",
       "1    [ Young savers' ] parents and grandparents are...\n",
       "2    However, we cannot share any specific details ...\n",
       "3    HUD did not drop the complaint, but insisted t...\n",
       "4    2014 was a milestone year for us on many front...\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first few quotations as a sanity check that the data were loaded correctly\n",
    "df['quotation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a507de91-b7e0-4eca-93d1-cd0bd0f17295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782  \n",
       "1  young saver parent grandpar use higher interes...           0.000000  \n",
       "2  howev share specif detail stage we provid info...           0.170075  \n",
       "3        hud drop complaint insist still issu code\\n           0.286240  \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data to see if the loading worked as expected\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed63e30-4bdd-4aa4-aa86-3d498fdc660d",
   "metadata": {},
   "source": [
    "### BERT pre-trained sentiment classifier\n",
    "The first model we try to employ in order to perform the sentiment classification of our unlabled set of quotations is the pre-trained BERT model. To do so we emply the transformers library which uses tensorflow and which gives direct access to the `bert-based-incased` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0afdee-4474-4e26-b3b0-5cf876c90cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 17:41:49.657272: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-12 17:41:49.657527: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-12 17:41:49.657701: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noto.epfl.ch): /proc/driver/nvidia/version does not exist\n",
      "2021-11-12 17:41:49.660042: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained BERT model as well as the required tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929df3ab-797d-4277-a80e-abcc0851879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# display the properties of the laoded BERT model as sanity check that the loading of the model functioned correctly\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fa0542-4a3a-46f4-a9ec-1c40421fdc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\n",
      "\n",
      "  Tokens: ['are', 'we', 'able', 'to', 'predict', 'the', 'stock', 'market', 'using', 'the', 'sentiment', 'expressed', 'by', 'famous', 'people', '?', 'ada', '##del', '##ta', '-', 'q', 'is', 'here', 'to', 'answer', 'your', 'question']\n",
      "\n",
      "  Token IDs: [2024, 2057, 2583, 2000, 16014, 1996, 4518, 3006, 2478, 1996, 15792, 5228, 2011, 3297, 2111, 1029, 15262, 9247, 2696, 1011, 1053, 2003, 2182, 2000, 3437, 2115, 3160]\n"
     ]
    }
   ],
   "source": [
    "# Test if the tokenizer works\n",
    "sample_txt = \"Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\"\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a18a4c9-39ea-4f7b-880c-ad98c88f2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: However, due to sharp decline in KG-D6 gas production not only could gas not be allocated, to new gas-based projects, but the commissioned capacity that KG-D6 gas allocation also get stranded,\n",
      "\n",
      "  Tokens: ['however', ',', 'due', 'to', 'sharp', 'decline', 'in', 'kg', '-', 'd', '##6', 'gas', 'production', 'not', 'only', 'could', 'gas', 'not', 'be', 'allocated', ',', 'to', 'new', 'gas', '-', 'based', 'projects', ',', 'but', 'the', 'commissioned', 'capacity', 'that', 'kg', '-', 'd', '##6', 'gas', 'allocation', 'also', 'get', 'stranded', ',']\n",
      "\n",
      "  Token IDs: [2174, 1010, 2349, 2000, 4629, 6689, 1999, 4705, 1011, 1040, 2575, 3806, 2537, 2025, 2069, 2071, 3806, 2025, 2022, 11095, 1010, 2000, 2047, 3806, 1011, 2241, 3934, 1010, 2021, 1996, 4837, 3977, 2008, 4705, 1011, 1040, 2575, 3806, 16169, 2036, 2131, 15577, 1010]\n"
     ]
    }
   ],
   "source": [
    "#Try tokenization of one sentence in dataframe\n",
    "sample_txt = df.iloc[0]['quotation']\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48af9c-c748-4900-895d-5669f350615c",
   "metadata": {},
   "source": [
    "Since running the BERT model can be very costly in terms of running time, we decide to test its performance on the sentiment classification task, WITHOUT fine-tuning it, by using three artificial sentences whose polarities are relatively obvious (to a human) and span all the possible polarity categories (positive, negative, and neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff12d01-e706-4a51-aa4e-6b99f5e709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two artifical sentences\n",
    "pred_sentences = ['The performance of that company was incredible, the revenue will be increasing constantly over the next years',\n",
    "                  'Their financial stability has been questioned multiple times, they are on the edge of bankruptcy',\n",
    "                  'The market will probably be unaffected by these events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a10710-d49e-48b4-863b-746c3f16320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.33168676 0.6683132 ]\n",
      " [0.32882562 0.67117435]\n",
      " [0.41989222 0.5801078 ]], shape=(3, 2), dtype=float32)\n",
      "The performance of that company was incredible, the revenue will be increasing constantly over the next years : \n",
      " Positive\n",
      "Their financial stability has been questioned multiple times, they are on the edge of bankruptcy : \n",
      " Positive\n",
      "The market will probably be unaffected by these events : \n",
      " Positive\n"
     ]
    }
   ],
   "source": [
    "# Get the sentiment prediction using the pre_trained BERT model\n",
    "tokenized_sentences = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "outputs = model(tokenized_sentences)\n",
    "\n",
    "# use softmax to obtain values interpretable as probabilities\n",
    "sentiments = tf.nn.softmax(outputs[0], axis=-1)\n",
    "print(sentiments)\n",
    "\n",
    "# print the obtained sentiment classification \n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(sentiments, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2a2d2-7129-4231-a426-a57b48c8a02b",
   "metadata": {},
   "source": [
    "We notice that even with very simple and relatively obvious sentences, the pre-trained BERT model does not seem to detect the correct polarity. This is not surprising since it has been shown in the literature that fine-tuning is a critical step in order to be able to employ this pre-trained classifier. A possible solution would be to manually label a certain amount of quotations that wil be used to fine-tune the BERT model. However, this soltuon would be very time-consuming, therefore we firt explore other unsupervised methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd128d-4f59-40c3-99d8-4c3f9a0acb54",
   "metadata": {},
   "source": [
    "### VADER unsupervised sentiment classifier\n",
    "\n",
    "VADER is a famous unsupervised sentiment classifier that can be imported from the NLTK library. It makes use of a lexicon where each word is associated with a sentiment score and the the sentence sentiment is obtained by aggregating the word-specific polarities. \n",
    "It is an interesting method to explore because it is very easy to implement and does not require any label as an input. On the other side, though, its sentiment classification results have been shown to be quite poor. Once again, to test this we use the artificial sentences we have create before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db3ddda-0adc-4f3c-b449-da1c1d2d5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09925a42-4db6-46eb-a330-906e4935f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the VADER sentiment classifier\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    if scores[\"neg\"] > scores[\"pos\"] and scores[\"neg\"] > scores[\"pos\"]:\n",
    "        return 'negative'\n",
    "    elif scores[\"pos\"] > scores[\"neg\"] and scores[\"pos\"] > scores[\"neu\"]:\n",
    "        return 'positive'\n",
    "    return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ef38e5-a1af-4e5d-891f-c6667c32c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of that company was incredible, the revenue will be increasing constantly over the next years : \n",
      " neutral\n",
      "Their financial stability has been questioned multiple times, they are on the edge of bankruptcy : \n",
      " negative\n",
      "The market will probably be unaffected by these events : \n",
      " neutral\n"
     ]
    }
   ],
   "source": [
    "for sentence in pred_sentences:\n",
    "    sentiment = vader_sentiment_result(sentence)\n",
    "    print(sentence, \": \\n\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8976489-93de-462f-a666-4039708fd510",
   "metadata": {},
   "source": [
    "It can be noticed that VADER is able to detect the negative polarity expresed in one sentence, but also this model does not perform as well as we hoped for. Having as precise as possible sentiments is key to avoid biasing the results of the projects significnatly. We therefore look further for another solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d04475-e488-4f28-92e6-d67c2ffed0a6",
   "metadata": {},
   "source": [
    "### Zero-shot classifier\n",
    "\n",
    "In the past years, a large attention has been dedicated to the so called \"Zero-Shot NLP models\", which are models pre-trained on different tasks but that are able to perform well on unseen and unlabled data without needing fine-tuning, which is what we need! We use the Transformers library which employs models from the Hugging Face hub. The models available in the Transformers library are trained using the Natural Language Inference (NLI) approach, and thus require for each sentence a premise and an hypothesis to be tested. In our case the hypothesis are the sentiment and thus we set the hypothesis to be \"The sentiment of this quote is positive/neutral/negative\". As before, we use the three artificial sentences previously created in order to have a first quick analysis of the classification perfromance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4c0b5-c3cf-4225-9059-7d2a625afd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize the zer-shot classifier (it will use the default model robert-large-mnli)\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Crete the hypothesis we want to use\n",
    "hypotheses = \"The sentiment of this quote is {}.\"\n",
    "\n",
    "# Create the labels\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b893a1-f74a-4f10-ad10-505d6390747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'The performance of that company was incredible, the revenue will be increasing constantly over the next years', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9952166080474854, 0.3805881142616272, 0.0035103177651762962]} \n",
      " The predicted  sentiment is:  positive\n",
      "{'sequence': 'Their financial stability has been questioned multiple times, they are on the edge of bankruptcy', 'labels': ['negative', 'neutral', 'positive'], 'scores': [0.9933288097381592, 0.11964941024780273, 0.005131530575454235]} \n",
      " The predicted  sentiment is:  negative\n",
      "{'sequence': 'The market will probably be unaffected by these events', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9269043207168579, 0.8344512581825256, 0.01127589587122202]} \n",
      " The predicted  sentiment is:  positive\n"
     ]
    }
   ],
   "source": [
    "for sentence in pred_sentences:\n",
    "    sentiment = classifier(sentence, the_labels, hypothesis_template = hypotheses, multi_label=True)\n",
    "    print(sentiment, \"\\n\", 'The predicted  sentiment is: ', sentiment['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e4e92-ec1b-4d57-8f30-22679590fe3c",
   "metadata": {},
   "source": [
    "The results above show that this model is better than the pre-trained BERT and The VADER models. It is able to correctly detect the negative and positive sentiments. The sentence that has a more neutral senitment was categroized as positive. The more neutral sentence was categorized as positive, however we notice that the score assigned to the neutral class was also relatively high. Moreover, detecting neutral sentence has always been the most difficult part for sentiment classifiers since it is sometimes very hard to draw a fine line between was is positive/negative and what is neutral. It has been shown that the selection of the working of the hypothesis has an influence on the performance so we might need to compare different formulations as well.\n",
    "\n",
    "Given the exeperiments above, we decide to explore fruther only the VADER and the Zero-SHot classifiers. To do so, we run both of them on 1000 random quotations from those filtered from 2015 and we then manually go through them to see which classifier resonates more with what a human would choose. The resulting dataframe is saved within `data` for possible future purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5b5f84-d57a-4bdf-a3c4-4d0b67b082a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [2:43:14,  9.79s/it]\n"
     ]
    }
   ],
   "source": [
    "hypothesis_template = \"The sentiment of this quote is {}.\"\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "vader_sent = []\n",
    "one_shot_sent = []\n",
    "\n",
    "# For each sentence, predict the sentiment using both VADER and Zero-Shot and save the sentiment in a column of the dataframe\n",
    "for idx, item in tqdm(df.iterrows()):\n",
    "    vader_sent.append(vader_sentiment_result(item['quotation']))\n",
    "    one_shot_sent.append(classifier(item['quotation'], the_labels, hypothesis_template = hypotheses, multi_label=True)['labels'][0])\n",
    "\n",
    "df['vader sentiment'] = vader_sent\n",
    "df['one-shot sentiment'] = one_shot_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3019291f-a4fb-4681-9a6b-80fb099ee528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>vader sentiment</th>\n",
       "      <th>one-shot sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \\\n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782   \n",
       "1  young saver parent grandpar use higher interes...           0.000000   \n",
       "2  howev share specif detail stage we provid info...           0.170075   \n",
       "3        hud drop complaint insist still issu code\\n           0.286240   \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876   \n",
       "\n",
       "  vader sentiment one-shot sentiment  \n",
       "0         neutral           negative  \n",
       "1         neutral           positive  \n",
       "2         neutral           negative  \n",
       "3         neutral           negative  \n",
       "4         neutral           positive  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check, print the first few quotations\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9934e7c-a71e-4ac6-ab0e-4ca91425df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For possible future purposes, we save the results \n",
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='sentiment_analysis_test.csv')  \n",
    "df.to_csv('sentiment_analysis_test.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899d84c-0e3f-4432-8c2a-ddcbdbf92270",
   "metadata": {},
   "source": [
    "\n",
    "Nevertheless, using the same 1000 quotations we used for testing VADER and using the default model roberta-large-mnli, we found a more intuitive and reasonable classification of the sentences' polarities. We will thus use this method to classify all the quotations. The running time will be significant (it took approximately 1 hour for 1000 sentences), but since we only need to do it once it is feasible.\n",
    "\n",
    "After analyzing the results, we find that the Zero-shot model leads to a more intuitive and reasonable classification of the sentences' polarities and thus we decide to emply the Zero-shot classifier as our final method. It should be kept in mind that this sentiments are provided by an algorithm, thus we acknowledge that the results that will follow might be afected by possible biases created in this step. To (partially) avoid such biases we would need to do the sentiment classification ourselves, which would be unreasonable in the given time avaiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f0fbd-1586-492f-884b-2a237ca30ede",
   "metadata": {},
   "source": [
    "### Final Sentiment Detetion \n",
    "\n",
    "The next (and last) step is to run the same code we used above on all the quotations we would like to use fro the project. This step has not been performed yet because we noticed a bug in the filtering procedure and thus we need to rerun it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507ad68-b302-4705-bc6a-a860136dc4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
