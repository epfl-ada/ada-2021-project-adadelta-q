{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e51ed2-cf8b-4f47-8b1a-da100de6e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 17:09:41.687551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 17:09:41.687589: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries. Note, in order to be able to run this cell you must have installed the libraires listed \n",
    "# in the requirements.txt file\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from importlib import reload\n",
    "# from src import embeddings_filter\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668622b-798a-4e53-a70c-eee485973db3",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "In the pipeline, the sentiment classification task is after filtering out irrelevant sentences. We thus need to load the dataset back into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496598ff-1154-4a31-94a5-0132131b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "k = 0\n",
    "with bz2.open('data/final_filtered.json.bz2', 'rb') as s_file:\n",
    "    while True:\n",
    "        try:\n",
    "            d =  json.loads(next(s_file))\n",
    "            # print(d['quotation'])\n",
    "            sentences.append(d)\n",
    "            k+=1\n",
    "            if k == 1000:\n",
    "                break\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae67b9fe-c9d4-445d-a12a-828d1a7c7ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    However, due to sharp decline in KG-D6 gas pro...\n",
       "1    [ Young savers' ] parents and grandparents are...\n",
       "2    However, we cannot share any specific details ...\n",
       "3    HUD did not drop the complaint, but insisted t...\n",
       "4    2014 was a milestone year for us on many front...\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first few quotations as a sanity check that the data were loaded correctly\n",
    "df['quotation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a507de91-b7e0-4eca-93d1-cd0bd0f17295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782  \n",
       "1  young saver parent grandpar use higher interes...           0.000000  \n",
       "2  howev share specif detail stage we provid info...           0.170075  \n",
       "3        hud drop complaint insist still issu code\\n           0.286240  \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data to see if the loading worked as expected\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed63e30-4bdd-4aa4-aa86-3d498fdc660d",
   "metadata": {},
   "source": [
    "### BERT pre-trained sentiment classifier\n",
    "The first model we try to employ in order to perform the sentiment classification of our unlabled set of quotations is the pre-trained BERT model. To do so we emply the transformers library which uses tensorflow and which gives direct access to the `bert-based-incased` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0afdee-4474-4e26-b3b0-5cf876c90cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 16:28:17.851254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-08 16:28:17.851470: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-08 16:28:17.851584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noto.epfl.ch): /proc/driver/nvidia/version does not exist\n",
      "2021-11-08 16:28:17.852519: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained BERT model as well as the required tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929df3ab-797d-4277-a80e-abcc0851879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# display the properties of the laoded BERT model as sanity check that the loading of the model functioned correctly\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fa0542-4a3a-46f4-a9ec-1c40421fdc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\n",
      "\n",
      "  Tokens: ['are', 'we', 'able', 'to', 'predict', 'the', 'stock', 'market', 'using', 'the', 'sentiment', 'expressed', 'by', 'famous', 'people', '?', 'ada', '##del', '##ta', '-', 'q', 'is', 'here', 'to', 'answer', 'your', 'question']\n",
      "\n",
      "  Token IDs: [2024, 2057, 2583, 2000, 16014, 1996, 4518, 3006, 2478, 1996, 15792, 5228, 2011, 3297, 2111, 1029, 15262, 9247, 2696, 1011, 1053, 2003, 2182, 2000, 3437, 2115, 3160]\n"
     ]
    }
   ],
   "source": [
    "# Test if the tokenizer works\n",
    "sample_txt = \"Are we able to predict the stock market using the sentiment expressed by famous people? Adadelta-Q is here to answer your question\"\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a18a4c9-39ea-4f7b-880c-ad98c88f2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentence: However, due to sharp decline in KG-D6 gas production not only could gas not be allocated, to new gas-based projects, but the commissioned capacity that KG-D6 gas allocation also get stranded,\n",
      "\n",
      "  Tokens: ['however', ',', 'due', 'to', 'sharp', 'decline', 'in', 'kg', '-', 'd', '##6', 'gas', 'production', 'not', 'only', 'could', 'gas', 'not', 'be', 'allocated', ',', 'to', 'new', 'gas', '-', 'based', 'projects', ',', 'but', 'the', 'commissioned', 'capacity', 'that', 'kg', '-', 'd', '##6', 'gas', 'allocation', 'also', 'get', 'stranded', ',']\n",
      "\n",
      "  Token IDs: [2174, 1010, 2349, 2000, 4629, 6689, 1999, 4705, 1011, 1040, 2575, 3806, 2537, 2025, 2069, 2071, 3806, 2025, 2022, 11095, 1010, 2000, 2047, 3806, 1011, 2241, 3934, 1010, 2021, 1996, 4837, 3977, 2008, 4705, 1011, 1040, 2575, 3806, 16169, 2036, 2131, 15577, 1010]\n"
     ]
    }
   ],
   "source": [
    "#Try tokenization of one sentence in dataframe\n",
    "sample_txt = df.iloc[0]['quotation']\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'  Sentence: {sample_txt}')\n",
    "print(f'\\n  Tokens: {tokens}')\n",
    "print(f'\\n  Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48af9c-c748-4900-895d-5669f350615c",
   "metadata": {},
   "source": [
    "Since running the BERT model can be very costly in terms of running time, we decide to test its performance on the sentiment classification task, WITHOUT fine-tuning it, by using three artificial sentences whose polarities are relatively obvious (to a human) and span all the possible polarity categories (positive, negative, and neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aff12d01-e706-4a51-aa4e-6b99f5e709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two artifical sentences\n",
    "pred_sentences = ['The performance of that company was incredible, the revenue will be increasing constantly over the next years',\n",
    "                  'Their financial stability has been questioned multiple times, they are on the edge of bankruptcy',\n",
    "                  'The market will probably be unaffected by these events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a10710-d49e-48b4-863b-746c3f16320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.39626688 0.60373306]\n",
      " [0.4094616  0.59053844]], shape=(2, 2), dtype=float32)\n",
      "This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n",
      " Positive\n",
      "One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie : \n",
      " Positive\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "print(tf_predictions)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db3ddda-0adc-4f3c-b449-da1c1d2d5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09925a42-4db6-46eb-a330-906e4935f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    if scores[\"neg\"] > scores[\"pos\"] and scores[\"neg\"] > scores[\"pos\"]:\n",
    "        return 'negative'\n",
    "    elif scores[\"pos\"] > scores[\"neg\"] and scores[\"pos\"] > scores[\"neu\"]:\n",
    "        return 'positive'\n",
    "    return 'neutral'\n",
    "\n",
    "# labels = vader_sentiment_result(pred_sentences)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     print(pred_sentences[i], \": \\n\", vader_sentiment_result(pred_sentences[i]))\n",
    "# train_set[\"vader_result\"] = train_set[\"review\"].apply(lambda x: vader_sentiment_result(x))\n",
    "# valid_set[\"vader_result\"] = valid_set[\"review\"].apply(lambda x: vader_sentiment_result(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf4c0b5-c3cf-4225-9059-7d2a625afd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d19bf5f3ef41999b13c56f09a571ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f787b2a81642d69ba732d829659034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317cf76db9764277905432c821355585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:02<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d95cbb72344b5daa28e1af65ceb8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4c65ab44fe4329b501bd4a313f8894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "a_review = pred_sentences[1]\n",
    "\n",
    "res = classifier(a_review, the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1251dc-fadb-4e56-b4ee-9429c68c7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie', 'labels': ['negative', 'positive'], 'scores': [0.9947044253349304, 0.005295595154166222]}\n"
     ]
    }
   ],
   "source": [
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5b5f84-d57a-4bdf-a3c4-4d0b67b082a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [2:43:14,  9.79s/it]\n"
     ]
    }
   ],
   "source": [
    "hypothesis_template = \"The sentiment of this quote is {}.\"\n",
    "the_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "vader_sent = []\n",
    "one_shot_sent = []\n",
    "\n",
    "for idx, item in tqdm(df.iterrows()):\n",
    "    vader_sent.append(vader_sentiment_result(item['quotation']))\n",
    "    one_shot_sent.append(classifier(item['quotation'], the_labels, hypothesis_template = hypothesis_template, multi_label=True)['labels'][0])\n",
    "\n",
    "df['vader sentiment'] = vader_sent\n",
    "df['one-shot sentiment'] = one_shot_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3019291f-a4fb-4681-9a6b-80fb099ee528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>vader sentiment</th>\n",
       "      <th>one-shot sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-26-025269</td>\n",
       "      <td>However, due to sharp decline in KG-D6 gas pro...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>[Q7199798]</td>\n",
       "      <td>2015-03-26 10:02:46</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://timesofindia.indiatimes.com/business/i...</td>\n",
       "      <td>howev due sharp declin kgd6 ga product could g...</td>\n",
       "      <td>0.251782</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28-001053</td>\n",
       "      <td>[ Young savers' ] parents and grandparents are...</td>\n",
       "      <td>Patrick Connolly</td>\n",
       "      <td>[Q7146267]</td>\n",
       "      <td>2015-10-28 07:26:15</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://gulfnews.com/business/sectors/features...</td>\n",
       "      <td>young saver parent grandpar use higher interes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-19-025137</td>\n",
       "      <td>However, we cannot share any specific details ...</td>\n",
       "      <td>David Kalisch</td>\n",
       "      <td>[Q26322384]</td>\n",
       "      <td>2015-02-19 00:26:19</td>\n",
       "      <td>5</td>\n",
       "      <td>[http://www.smh.com.au/federal-politics/politi...</td>\n",
       "      <td>howev share specif detail stage we provid info...</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-16-013190</td>\n",
       "      <td>HUD did not drop the complaint, but insisted t...</td>\n",
       "      <td>Dennis Wheeler</td>\n",
       "      <td>[Q55219988]</td>\n",
       "      <td>2015-05-16 21:23:57</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://adn.com/article/20150514/anchorage-cha...</td>\n",
       "      <td>hud drop complaint insist still issu code\\n</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-13-000112</td>\n",
       "      <td>2014 was a milestone year for us on many front...</td>\n",
       "      <td>Mike Fries</td>\n",
       "      <td>[Q54861319]</td>\n",
       "      <td>2015-02-13 13:01:44</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://advanced-television.com/2015/02/13/lib...</td>\n",
       "      <td>2014 mileston year us mani front we increas pa...</td>\n",
       "      <td>0.280876</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-26-025269  However, due to sharp decline in KG-D6 gas pro...   \n",
       "1  2015-10-28-001053  [ Young savers' ] parents and grandparents are...   \n",
       "2  2015-02-19-025137  However, we cannot share any specific details ...   \n",
       "3  2015-05-16-013190  HUD did not drop the complaint, but insisted t...   \n",
       "4  2015-02-13-000112  2014 was a milestone year for us on many front...   \n",
       "\n",
       "            speaker         qids                 date  numOccurrences  \\\n",
       "0      Piyush Goyal   [Q7199798]  2015-03-26 10:02:46               1   \n",
       "1  Patrick Connolly   [Q7146267]  2015-10-28 07:26:15               1   \n",
       "2     David Kalisch  [Q26322384]  2015-02-19 00:26:19               5   \n",
       "3    Dennis Wheeler  [Q55219988]  2015-05-16 21:23:57               1   \n",
       "4        Mike Fries  [Q54861319]  2015-02-13 13:01:44               1   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://timesofindia.indiatimes.com/business/i...   \n",
       "1  [http://gulfnews.com/business/sectors/features...   \n",
       "2  [http://www.smh.com.au/federal-politics/politi...   \n",
       "3  [http://adn.com/article/20150514/anchorage-cha...   \n",
       "4  [http://advanced-television.com/2015/02/13/lib...   \n",
       "\n",
       "                                           tokenized  cosine_similarity  \\\n",
       "0  howev due sharp declin kgd6 ga product could g...           0.251782   \n",
       "1  young saver parent grandpar use higher interes...           0.000000   \n",
       "2  howev share specif detail stage we provid info...           0.170075   \n",
       "3        hud drop complaint insist still issu code\\n           0.286240   \n",
       "4  2014 mileston year us mani front we increas pa...           0.280876   \n",
       "\n",
       "  vader sentiment one-shot sentiment  \n",
       "0         neutral           negative  \n",
       "1         neutral           positive  \n",
       "2         neutral           negative  \n",
       "3         neutral           negative  \n",
       "4         neutral           positive  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9934e7c-a71e-4ac6-ab0e-4ca91425df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='out.csv')  \n",
    "df.to_csv('out.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63f6fe-3896-4eb1-9098-3163b95d997e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
